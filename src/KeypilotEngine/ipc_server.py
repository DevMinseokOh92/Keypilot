import time
import win32pipe, win32file, pywintypes
import torch
from unsloth import FastLanguageModel

# ==========================================
# [ì„¤ì •] íŒŒì´í”„ ì´ë¦„ (C#ì´ë‘ ë˜‘ê°™ì•„ì•¼ í•¨)
# ==========================================
PIPE_NAME = r'\\.\pipe\keypilot_pipe'
MODEL_ID = "meta-llama/Llama-3.2-1B-Instruct"

def load_model():
    print("ğŸ”„ [Init] ëª¨ë¸ ë¡œë”© ì¤‘... (4-bit)")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = MODEL_ID,
        max_seq_length = 2048,
        dtype = None,
        load_in_4bit = True,
    )
    FastLanguageModel.for_inference(model)
    print("âœ… [Init] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì—°ê²° ëŒ€ê¸° ì¤‘...")
    return model, tokenizer

def run_server():
    model, tokenizer = load_model()
    
    print(f"ğŸ“¡ [Server] íŒŒì´í”„ ìƒì„±: {PIPE_NAME}")
    
    while True:
        try:
            # 1. íŒŒì´í”„ ìƒì„± (Named Pipe)
            pipe = win32pipe.CreateNamedPipe(
                PIPE_NAME,
                win32pipe.PIPE_ACCESS_DUPLEX, # ì–‘ë°©í–¥ í†µì‹ 
                win32pipe.PIPE_TYPE_MESSAGE | win32pipe.PIPE_READMODE_MESSAGE | win32pipe.PIPE_WAIT,
                1, 65536, 65536, 0, None
            )
            
            # 2. í´ë¼ì´ì–¸íŠ¸(C#) ì ‘ì† ëŒ€ê¸° (ì—¬ê¸°ì„œ ë©ˆì¶°ìˆìŒ)
            print("â³ [Wait] í´ë¼ì´ì–¸íŠ¸ ì ‘ì† ëŒ€ê¸° ì¤‘...")
            win32pipe.ConnectNamedPipe(pipe, None)
            print("ğŸ”— [Connect] í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨!")

            # 3. ë°ì´í„° ìˆ˜ì‹  (Read)
            # C#ì—ì„œ ë³´ë‚¸ í…ìŠ¤íŠ¸ ì½ê¸°
            resp = win32file.ReadFile(pipe, 64*1024)
            user_input = resp[1].decode('utf-8')
            print(f"ğŸ“© [Recv] ë°›ì€ ë‚´ìš©: {user_input}")

            # 4. AI ì¶”ë¡  (Generate)
            # (ì†ë„ë¥¼ ìœ„í•´ ê°„ë‹¨í•œ í…œí”Œë¦¿ ì ìš©)
            messages = [{"role": "user", "content": user_input}]
            inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")
            
            outputs = model.generate(inputs, max_new_tokens=50, use_cache=True) # ì†ë„ ìœ„í•´ 50í† í°ë§Œ ìƒì„±
            response_text = tokenizer.batch_decode(outputs)[0].split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
            
            print(f"ğŸ“¤ [Send] ë³´ë‚¼ ë‚´ìš©: {response_text[:30]}...") # ë¡œê·¸ì—” ì•ë¶€ë¶„ë§Œ ì¶œë ¥

            # 5. ë°ì´í„° ì†¡ì‹  (Write)
            # C#ìœ¼ë¡œ ê²°ê³¼ ë³´ë‚´ê¸°
            win32file.WriteFile(pipe, response_text.encode('utf-8'))

        except pywintypes.error as e:
            if e.args[0] == 109: # Broken Pipe
                print("âŒ [Error] í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€")
            else:
                print(f"âŒ [Error] íŒŒì´í”„ ì—ëŸ¬: {e}")
        finally:
            # ì—°ê²° í•´ì œ í›„ ë‹¤ì‹œ ëŒ€ê¸° ë£¨í”„
            win32file.CloseHandle(pipe)

if __name__ == "__main__":
    run_server()