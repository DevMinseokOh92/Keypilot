import os
# 1. Triton ë„ê¸°
os.environ["UNSLOTH_USE_TRITON"] = "0" 

import torch

# ================= [â˜… ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”] =================
# ì´ì „ lambda ì½”ë“œëŠ” @torch.compile(ì˜µì…˜) í˜•íƒœë¥¼ ì²˜ë¦¬ ëª»í–ˆìŠµë‹ˆë‹¤.
# ì•„ë˜ í•¨ìˆ˜ëŠ” ì–´ë–¤ í˜•íƒœë¡œ í˜¸ì¶œë˜ë“  ë¬´ì¡°ê±´ "ê·¸ëƒ¥ í†µê³¼ì‹œì¼œ!"ë¼ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.

def no_op_compile(source_fn=None, *args, **kwargs):
    # Case A: @torch.compile(dynamic=True) ì²˜ëŸ¼ ì˜µì…˜ë§Œ ì£¼ê³  í˜¸ì¶œí–ˆì„ ë•Œ
    if source_fn is None:
        return lambda x: x # ë‚˜ì¤‘ì— í•¨ìˆ˜ê°€ ë“¤ì–´ì˜¤ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì¤Œ
    # Case B: torch.compile(model) ì²˜ëŸ¼ ë°”ë¡œ í˜¸ì¶œí–ˆì„ ë•Œ
    return source_fn # ë“¤ì–´ì˜¨ ê±¸ ê·¸ëŒ€ë¡œ ë°˜í™˜

torch.compile = no_op_compile
# =========================================================

import time
from unsloth import FastLanguageModel

# ==========================================
# ğŸ‘‡ ì—¬ê¸°ì„œ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì€ ëª¨ë¸ì˜ ì£¼ì„(#)ì„ í‘¸ì„¸ìš”
# ==========================================

# 1. Meta Llama 3.2 (1B) - [ì†ë„ 1ìœ„ / ë°¸ëŸ°ìŠ¤]
# model_name = "meta-llama/Llama-3.2-1B-Instruct"

# 2. Qwen 2.5 Coder (1.5B) - [ì½”ë”© 1ìœ„]
# model_name = "unsloth/Qwen2.5-Coder-1.5B-Instruct"

# 3. Google Gemma 2 (2B) - [ë¬¸ì¥ë ¥ ìš°ìˆ˜ / ì•½ê°„ ë¬´ê±°ì›€]
model_name = "unsloth/gemma-2-2b-it"

# 4. MS Phi-3.5 (3.8B) - [ë˜‘ë˜‘í•¨ / ì•„ì£¼ ë¬´ê±°ì›€]
# model_name = "unsloth/Phi-3.5-mini-instruct"

# ==========================================

max_seq_length = 2048
load_in_4bit = True

print(f"\nğŸ”„ [ì‹œì‘] {model_name} ë¡œë“œ ì¤‘...")

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = load_in_4bit,
)
FastLanguageModel.for_inference(model)

# â˜… í•µì‹¬ ë³€ê²½: ëª¨ë¸ì— ë§ëŠ” ëŒ€í™” í˜•ì‹ì„ ìë™ìœ¼ë¡œ ì ìš©í•´ì¤Œ
messages = [
    {"role": "user", "content": "C++ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ í•¨ìˆ˜ë¥¼ ì§œê³  ì›ë¦¬ë¥¼ ê¸¸ê²Œ ì„¤ëª…í•´ì¤˜."}
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # AIê°€ ë‹µë³€í•  ì°¨ë¡€ì„ì„ ì•Œë ¤ì¤Œ
    return_tensors = "pt",
).to("cuda")

print("\nğŸ§  [í…ŒìŠ¤íŠ¸] ìƒì„± ì‹œì‘ (Warm-up)...")
model.generate(inputs, max_new_tokens = 10, use_cache = True)

print("ğŸš€ [ì¸¡ì •] ì§„ì§œ ì†ë„ ì¸¡ì • ì‹œì‘...")
torch.cuda.synchronize()
start_time = time.time()

# ìµœëŒ€ 500 í† í° ìƒì„±
outputs = model.generate(inputs, max_new_tokens = 500, use_cache = True)

torch.cuda.synchronize()
end_time = time.time()

# ìƒì„±ëœ í† í° ê°œìˆ˜ (ì „ì²´ - ì§ˆë¬¸ ê¸¸ì´)
generated_tokens = outputs.shape[1] - inputs.shape[1]
duration = end_time - start_time
tps = generated_tokens / duration

# ë‹µë³€ í™•ì¸ (ì œëŒ€ë¡œ ë§í–ˆëŠ”ì§€ ì•ë¶€ë¶„ë§Œ ì¶œë ¥)
decoded_output = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)

print("\n" + "="*40)
print(f"ğŸ—£ï¸ [ë‹µë³€ ì¼ë¶€ í™•ì¸]: {decoded_output[:100]}...") 
print("="*40)

print(f"\nğŸ“Š [ìµœì¢… ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼: {model_name}]")
print(f"â€¢ ì´ ìƒì„± í† í° ìˆ˜: {generated_tokens} ê°œ")
print(f"â€¢ ê±¸ë¦° ì‹œê°„: {duration:.2f} ì´ˆ")
print(f"â€¢ âš¡ TPS (ì´ˆë‹¹ í† í° ìˆ˜): {tps:.2f} tokens/sec")
print("="*40)